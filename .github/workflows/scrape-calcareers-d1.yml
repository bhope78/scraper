name: CalCareers D1 Scraper

on:
  # Schedule to run daily at 2 AM UTC (6 PM PST / 7 PM PDT)
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      debug_enabled:
        type: boolean
        description: 'Run with debug logging'
        required: false
        default: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout for full scrape
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
    
    - name: Install dependencies
      run: |
        npm ci
        # Install wrangler globally
        npm install -g wrangler@latest
    
    - name: Install Playwright browsers
      run: npx playwright install chromium
    
    - name: Test D1 Connection
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      run: |
        echo "üîç Testing D1 connection..."
        node test-d1-connection.js
    
    - name: Run CalCareers Scraper
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        GITHUB_ACTIONS: 'true'  # This tells the scraper to run in headless mode
        NODE_ENV: production
        DEBUG: ${{ github.event.inputs.debug_enabled == 'true' && 'true' || 'false' }}
      run: |
        echo "üöÄ Starting CalCareers D1 scraper..."
        node playwright-windowed-scraper-d1.js
    
    - name: Generate Summary Report
      if: always()
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      run: |
        echo "üìä Generating summary report..."
        
        # Get job count from D1
        COUNT=$(npx wrangler d1 execute Calhr \
          --remote \
          --command "SELECT COUNT(*) as count FROM ccJobs" \
          --json 2>/dev/null | jq -r '.[0].results[0].count' || echo "Error getting count")
        
        # Get recent jobs
        RECENT=$(npx wrangler d1 execute Calhr \
          --remote \
          --command "SELECT job_control, working_title FROM ccJobs ORDER BY created_at DESC LIMIT 5" \
          --json 2>/dev/null | jq -r '.[0].results[] | "- \(.job_control): \(.working_title)"' || echo "Error getting recent jobs")
        
        # Create summary for GitHub Actions
        echo "## üìä CalCareers Scraper Run Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Run Date**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "**Total Jobs in Database**: $COUNT" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Recent Jobs Added:" >> $GITHUB_STEP_SUMMARY
        echo "$RECENT" >> $GITHUB_STEP_SUMMARY
    
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scraper-logs-${{ github.run_id }}
        path: |
          *.log
        retention-days: 30
    
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Scraper failed! Check the logs for details."
        # You can add additional notification logic here
        # For example, sending a webhook to Slack or Discord